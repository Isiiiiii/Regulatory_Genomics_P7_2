{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91ecafb2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "from loss_function import CrossEntropyNucleotideLoss\n",
    "from loss_mask import LossMask\n",
    "from model_architecture import FineTunedSpeciesLM\n",
    "from torch.utils.data import DataLoader\n",
    "from transformers import AutoTokenizer\n",
    "import loss_mask\n",
    "import numpy as np\n",
    "from gnomad_db.database import gnomAD_DB\n",
    "from proj_loader import prepare_data_loader\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "546b50e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_fine_tuned_model(\n",
    "    model,\n",
    "    data_loader,\n",
    "    device,\n",
    "    lossMask,\n",
    "    lr=1e-4,\n",
    "    epochs=5,\n",
    "    patience=5,\n",
    "    checkpoint_dir='checkpoints',\n",
    "    checkpoint_name='best_model.pt'\n",
    "):\n",
    "    \"\"\"\n",
    "    Trains FineTunedSpeciesLM with early stopping and TensorBoard logging using one-hot labels.\n",
    "    \"\"\"\n",
    "    os.makedirs(checkpoint_dir, exist_ok=True)\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=lr)\n",
    "    tb_writer = SummaryWriter(log_dir=checkpoint_dir)\n",
    "\n",
    "    loss_fn = CrossEntropyNucleotideLoss()\n",
    "    best_val_loss = float('inf')\n",
    "    epochs_no_improve = 0\n",
    "    model.to(device)\n",
    "\n",
    "    for epoch in range(1, epochs + 1):\n",
    "        # --- Training ---\n",
    "        model.train()\n",
    "        train_losses = []\n",
    "        pbar = tqdm(data_loader, desc=f\"Epoch {epoch}/{epochs} [train]\", ncols=120, leave=False)\n",
    "        \n",
    "        for batch in pbar:\n",
    "            input_ids = batch[\"input_ids\"].to(device)\n",
    "            labels = batch[\"labels\"].to(device)  # shape: [B, 2003, 4] (one-hot)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            print(f\"Batch input_ids shape: {input_ids.shape}\")\n",
    "            print(f\"Batch labels shape: {batch['labels'].shape}\")\n",
    "            print(f\"Batch labels max value: {batch['labels'].max().item()}\")\n",
    "            \n",
    "            outputs = model(input_ids=input_ids)  # shape: [B, 2003, 4]\n",
    "\n",
    "            print(f\"outputs shape: {outputs.shape}\")\n",
    "\n",
    "            pre_loss = loss_fn(outputs, labels)   # shape: [B, 2003]\n",
    "\n",
    "            #print(f\"pre_loss_shape: {pre_loss.shape}\")\n",
    "\n",
    "            #apply loss_mask\n",
    "            loss = lossMask.apply_mask(pre_loss, \"train\") # shape: scalar\n",
    "\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            train_losses.append(loss.item())\n",
    "\n",
    "        avg_train_loss = np.mean(train_losses)\n",
    "        tb_writer.add_scalar(\"train/loss\", avg_train_loss, epoch)\n",
    "\n",
    "        # --- Validation ---\n",
    "        model.eval()\n",
    "        val_losses = []\n",
    "        with torch.no_grad():\n",
    "            for batch in tqdm(data_loader, desc=f\"Epoch {epoch}/{epochs} [val]\", ncols=120, leave=False):\n",
    "                input_ids = batch[\"input_ids\"].to(device)\n",
    "                labels = batch[\"labels\"].to(device)\n",
    "\n",
    "                outputs = model(input_ids=input_ids)\n",
    "                pre_val_loss = loss_fn(outputs, labels)\n",
    "\n",
    "                #apply loss_mask\n",
    "                val_loss = lossMask.apply_mask(pre_val_loss, \"val\") # shape: scalar\n",
    "\n",
    "                val_losses.append(val_loss.item())\n",
    "\n",
    "        avg_val_loss = np.mean(val_losses)\n",
    "        tb_writer.add_scalar(\"val/loss\", avg_val_loss, epoch)\n",
    "\n",
    "        print(f\"Epoch {epoch}: train_loss = {avg_train_loss:.4f}, val_loss = {avg_val_loss:.4f}\")\n",
    "\n",
    "        # --- Checkpoint & Early Stopping ---\n",
    "        if avg_val_loss < best_val_loss:\n",
    "            best_val_loss = avg_val_loss\n",
    "            epochs_no_improve = 0\n",
    "            path = os.path.join(checkpoint_dir, checkpoint_name)\n",
    "            torch.save(model.state_dict(), path)\n",
    "            print(f\"  ↳ Saved best model at {path}\")\n",
    "        else:\n",
    "            epochs_no_improve += 1\n",
    "            if epochs_no_improve >= patience:\n",
    "                print(f\"  ↳ Early stopping after {patience} epochs without improvement.\")\n",
    "                break\n",
    "\n",
    "    tb_writer.close()\n",
    "    model.load_state_dict(torch.load(os.path.join(checkpoint_dir, checkpoint_name)))\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd10a5ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "# The model\n",
    "model = FineTunedSpeciesLM()\n",
    "\n",
    "# Initialize tokenizer and dataset\n",
    "data_loader = prepare_data_loader(batch_size=16)\n",
    "\n",
    "# Loss mask\n",
    "loss_mask = LossMask()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0574fd0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#trained_model = train_fine_tuned_model(\n",
    "#    model=model, \n",
    " #   data_loader=data_loader, \n",
    "  #  device=torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\"), \n",
    "   # lossMask=loss_mask)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc1b9f2d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ml4rg25_g7_2",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
